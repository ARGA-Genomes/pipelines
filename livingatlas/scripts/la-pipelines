#!/usr/bin/env bash

CMD=$(basename $0)

source ./set-env.sh

FIND_DOCOPTS=$(which docopts)
FIND_YQ=$(which yq)

if [[ -z $FIND_DOCOPTS ]]
then
    echo "ERROR: Please install docopts https://github.com/docopt/docopts an copy it in your PATH"
    exit 1
fi

if [[ -z $FIND_YQ ]]
then
    echo "ERROR: Please install yq, see https://github.com/mikefarah/yq"
    exit 1
fi

set -e # Stop on any failure

# Detect if we are in production or not
if [[ $PWD == "/usr/bin" ]] ; then PROD=true ; else PROD=false ; fi

if [[ $PROD = false ]]; then
    VER=$(grep -oPm1 "(?<=<version>)[^<]+" "../pom.xml")
else
    VER=$(dpkg-query --show -f '${Version}' la-pipelines)
fi

WHEREL="[--local|--embedded|--cluster]"
WHERE="[--embedded|--cluster]"

eval "$(docopts -V - -h - : "$@" <<EOF

LA-Pipelines data ingress utility.

Pipeline ingress steps:

    ┌───── do-all ──────────────────────────────────┐
    │                                               │
dwca-avro --> interpret --> uuid -->                │
     export-latlng --> sample --> sample-avro --> index

Usage:
  $CMD [options] dwca-avro     (<dr>...|all)
  $CMD [options] interpret     (<dr>...|all) $WHEREL
  $CMD [options] uuid          (<dr>...|all)         $WHERE
  $CMD [options] export-latlng (<dr>...|all)         $WHERE
  $CMD [options] sample        (<dr>...|all)
  $CMD [options] sample-avro   (<dr>...|all)         $WHERE
  $CMD [options] index         (<dr>...|all) $WHEREL
  $CMD [options] do-all        (<dr>...|all) $WHEREL
  $CMD -h | --help
  $CMD -v | --version

Options:
  --config=<files>     Comma separated list of alternative la-pipeline yaml configurations (the last file has the highest precedence).
  --extra-args=<args>  Additional "arg1=values,arg2=value" to pass to pipeline options (highest precedence than yml values).
  --no-colors          No corolorize logs output.
  --dry-run            Print the commands without actually running them.
  --debug              Debug $CMD.
  -h --help            Show this help.
  -v --version         Show version.
----
$CMD $VER
Copyright (C) 2020 ALA Development Team
License MPL v 1.1
EOF
)"

# Enable logging
if ($debug) ; then verbosity=6; else verbosity=5; fi
source ./logging_lib.sh $verbosity $no_colors $dr

log.info "Starting $CMD"

# TODO: externalize this CONFIG_DIR in some way, ask other locations
if [[ $PROD = true ]] ; then CONFIG_DIR=/data/la-pipelines/config ; else CONFIG_DIR=../configs; fi

if ($local); then unset EXTRA_CONFIG ; fi
if ($embedded); then EXTRA_CONFIG=,$CONFIG_DIR/la-pipelines-spark-embedded.yaml ; fi
if ($cluster); then EXTRA_CONFIG=,$CONFIG_DIR/la-pipelines-spark-cluster.yaml ; fi

if ($dry_run); then _D=echo; else _D=; fi
# Set default config locations for Production and Development
if [[ -z $config ]]; then config=$CONFIG_DIR/la-pipelines.yaml$EXTRA_CONFIG,$CONFIG_DIR/la-pipelines-local.yaml; fi

if [[ $PROD = false && $no_colors = false ]]; then logConfig=$PWD/../pipelines/src/main/resources/log4j-colorized.properties; fi
if [[ $PROD = false && $no_colors = true ]]; then logConfig=$PWD/../pipelines/src/main/resources/log4j.properties; fi
if [[ $PROD = true && $no_colors = false ]]; then logConfig=$CONFIG_DIR/log4j-colorized.properties; fi
if [[ $PROD = true && $no_colors = true ]]; then logConfig=$CONFIG_DIR/log4j.properties; fi

for dr in "${dr[@]}"; do
    if [[ -n $dr && $dr != all && $dr != dr* ]]; then >&2 log.error "Wrong dataResource '$dr'. It should start with 'dr', like 'dr893'"; exit 1 ; fi
done

if [[ -n $extra_args ]] ; then
    # Convert arg1=val1,arg2=val2 into --arg1=val1 --arg2=val2
    ARGS=${extra_args//,/ \-\-}
    ARGS=--${ARGS}
else
    ARGS=
fi

TYPE=local # by default
if ($local) ; then TYPE=local; fi
if ($embedded) ; then TYPE=spark-embedded; fi
if ($cluster) ; then TYPE=spark-cluster; fi

log.info Config: $config
log.info Extra arguments: $ARGS
log.debug Logs without colors: $no_colors
log.debug log4j config: $logConfig

# Convert config to a list of files space separated
configList=${config//,/ }
log.debug Config list: $configList

for f in $configList $logConfig
do
    if [[ ! -f $f ]] ; then log.error File $f doesn\'t exits ; exit 1; fi
done

# Gets a config value from yaml configList files
function getConf() {
    val=
    for i in $configList
    do
        valTmp=$(yq r $i $1)
        if [[ -n $valTmp ]] ; then val=$valTmp; fi
    done
    echo $val
}

function toArg() {
  PREFIX=$1
  KEY=$2
  echo --$KEY $(getConf $PREFIX.$KEY)
}

#log.debug Target $(getConf general.targetPath)
#PIPELINES_JAR=$(eval echo $(getConf java.PIPELINES_JAR))
#SPARK_TMP=$(getConf spark.SPARK_TMP)

log.debug Using $PIPELINES_JAR
log.debug Using $SPARK_TMP

# Uncomment this to debug log4j
# if ( $debug) ; then EXTRA_JVM_CLI_ARGS="-Dlog4j.debug"; fi

# Set log4j configuration for v1 and v2
EXTRA_JVM_CLI_ARGS="$EXTRA_JVM_CLI_ARGS -Dlog4j.configuration=file://$logConfig -Dlog4j.configurationFile=file://$logConfig"
log.debug EXTRA_JVM_CLI_ARGS: $EXTRA_JVM_CLI_ARGS

function logStepStart() {
    name=$1
    dr=$2
    log.info $(date)
    log.info "${colpur}$name${colrst} of $dr"
}

function logStepEnd() {
    name=$1
    dr=$2
    type=$3
    duration=$4
    log.info $(date)
    log.info "${colpur}$name${colrst} of $dr in [$type] took $(($duration / 60)) minutes and $(($duration % 60)) seconds."
}

function dwca-avro () {
    dr=$1

    SECONDS=0
    logStepStart "Ingest DwCA - Converts DwCA to verbatim.arvo file" $dr

    dwca_dir="/data/biocache-load/$dr/$dr.zip"

    if [[ ! -f  $dwca_dir ]]
    then
        dwca_dir="/data/biocache-load/$dr.zip"
    fi

    if [[ ! -f  $dwca_dir ]]
    then
        dwca_dir="/data/biocache-load/$dr.zip"
        log.error "$dwca_dir does not exists on your filesystem."
        exit 1
    fi

    $_D java $EXTRA_JVM_CLI_ARGS -Dspark.local.dir=$SPARK_TMP \
        -cp $PIPELINES_JAR au.org.ala.pipelines.beam.ALADwcaToVerbatimPipeline \
        --datasetId=$dr \
        --config=$config $ARGS

    logStepEnd dwca-avro $dr local $SECONDS
}

function interpret () {
    dr=$1
    ltype=$2
    CLASS=au.org.ala.pipelines.beam.ALAVerbatimToInterpretedPipeline

    SECONDS=0
    logStepStart "Interpret verbatim.arvo file" $dr

    PRE=interpret-sh-args.local
    if [[ $ltype = "local" ]] ; then
        $_D java $EXTRA_JVM_CLI_ARGS \
            $(getConf $PRE.jvm) \
            -cp $PIPELINES_JAR $CLASS \
            --datasetId=$dr \
            --config=$config
    fi

    PRE=interpret-sh-args.spark-embedded
    if [[ $ltype = "spark-embedded" ]] ; then
        log.info $(date)
        SECONDS=0
        $_D java $EXTRA_JVM_CLI_ARGS \
            $(getConf $PRE.jvm) \
            -cp $PIPELINES_JAR $CLASS \
            --datasetId=$dr \
            --config=$config
    fi

    PRE=interpret-sh-args.spark-cluster
    if [[ $ltype = "spark-cluster" ]] ; then
        $_D /data/spark/bin/spark-submit \
            --name "interpret $dr" \
            $(toArg $PRE conf) \
            $(toArg $PRE num-executors) \
            $(toArg $PRE executor-cores) \
            $(toArg $PRE executor-memory) \
            $(toArg $PRE driver-memory) \
            --class $CLASS \
            --master $SPARK_MASTER \
            --driver-java-options "-Dlog4j.configuration=file:/efs-mount-point/log4j.properties" \
            $PIPELINES_JAR \
            --datasetId=$dr \
            --config=$config
            # TODO set here an optional colorized log4j properties
    fi

    logStepEnd Interpretation $dr $ltype $SECONDS
}

function uuid () {
    dr=$1
    ltype=$2
    CLASS=au.org.ala.pipelines.beam.ALAUUIDMintingPipeline

    logStepStart "UUID pipeline - Preserve or add UUIDs." $dr
    SECONDS=0

    PRE=uuid-sh-args.spark-embedded
    if [[ $ltype = "spark-embedded" || $ltype = "local" ]] ; then
        # TODO: we can put this in a function? Same code that interpret
        $_D java $EXTRA_JVM_CLI_ARGS \
            $(getConf $PRE.jvm)  \
            -cp $PIPELINES_JAR $CLASS \
            --datasetId=$dr\
            --config=$config
    fi

    PRE=uuid-sh-args.spark-cluster
    if [[ $ltype = "spark-cluster" ]] ; then
        $_D /data/spark/bin/spark-submit \
            --name "uuid-minting $dr" \
            $(toArg $PRE num-executors) \
            $(toArg $PRE executor-cores) \
            $(toArg $PRE executor-memory) \
            $(toArg $PRE driver-memory) \
            --class $CLASS \
            --master $SPARK_MASTER \
            --driver-java-options "-Dlog4j.configuration=file:/efs-mount-point/log4j.properties" \
            $PIPELINES_JAR \
            --datasetId=$dr \
            --config=$config
            # TODO set here an optional colorized log4j properties
    fi

    logStepEnd "Adding uuids" $dr $ltype $SECONDS
}

function export-latlng () {
    dr=$1
    ltype=$2
    CLASS=au.org.ala.pipelines.beam.ALAInterpretedToLatLongCSVPipeline

    logStepStart "Export lat long" $dr
    SECONDS=0

    PRE=export-latlng-sh-args.spark-embedded
    if [[ $ltype = "spark-embedded" || $ltype = "local" ]] ; then
        $_D java $EXTRA_JVM_CLI_ARGS \
            $(getConf $PRE.jvm)  \
            -cp $PIPELINES_JAR $CLASS \
            --datasetId=$dr \
            --config=$config
    fi

    PRE=export-latlng-sh-args.spark-cluster
    if [[ $ltype = "spark-cluster" ]] ; then
        $_D /data/spark/bin/spark-submit \
            --name "Export $dr" \
            $(toArg $PRE num-executors) \
            $(toArg $PRE executor-cores) \
            $(toArg $PRE executor-memory) \
            $(toArg $PRE driver-memory) \
            --class $CLASS \
            --master $SPARK_MASTER \
            --driver-java -options "-Dlog4j.configuration=file:/efs-mount-point/log4j.properties" \
            $PIPELINES_JAR \
            --datasetId=$dr \
            --config=$config
            # TODO set here an optional colorized log4j properties
    fi

    logStepEnd "Export latlng" $dr $ltype $SECONDS
}

function sample () {
    dr=$1

    logStepStart "Sample" $dr
    SECONDS=0

    PRE=sample-sh-args.local
    $_D java $EXTRA_JVM_CLI_ARGS \
        $(getConf $PRE.jvm)  \
        -cp $PIPELINES_JAR au.org.ala.sampling.LayerCrawler \
        --datasetId=$dr \
        --config=$config

    logStepEnd "Sampling" $dr local $SECONDS
}

function sample-avro () {
    dr=$1
    ltype=$2
    CLASS=au.org.ala.pipelines.beam.ALASamplingToAvroPipeline

    logStepStart "Sample-avro" $dr
    SECONDS=0

    PRE=sample-avro-sh-args.spark-embedded
    if [[ $ltype = "spark-embedded" || $ltype = "local" ]] ; then
        $_D java $EXTRA_JVM_CLI_ARGS \
            $(getConf $PRE.jvm)  \
            -cp $PIPELINES_JAR $CLASS \
            --datasetId=$dr \
            --config=$config
    fi

    PRE=sample-avro-sh-args.spark-cluster
    if [[ $ltype = "spark-cluster" ]] ; then
        $_D /data/spark/bin/spark-submit \
            --name "add-sampling $dr" \
            $(toArg $PRE conf) \
            $(toArg $PRE num-executors) \
            $(toArg $PRE executor-cores) \
            $(toArg $PRE executor-memory) \
            $(toArg $PRE driver-memory) \
            --class $CLASS \
            --master $SPARK_MASTER \
            --driver-java-options "-Dlog4j.configuration=file:/efs-mount-point/log4j.properties" \
            $PIPELINES_JAR \
            --datasetId=$dr \
            --config=$config
            # TODO set here an optional colorized log4j properties
    fi

    logStepEnd "Sample-avro" $dr $ltype $SECONDS
}

function index () {
    dr=$1
    ltype=$2
    CLASS=au.org.ala.pipelines.java.ALAInterpretedToSolrIndexPipeline

    logStepStart "Indexing" $dr
    SECONDS=0

    PRE=index-sh-args.local
    if [[ $ltype = "local" ]] ; then
        $_D java $EXTRA_JVM_CLI_ARGS \
            $(getConf $PRE.jvm)  \
            -cp $PIPELINES_JAR $CLASS \
            --datasetId=$dr \
            --config=$config
    fi

    PRE=index-sh-args.spark-embedded
    if [[ $ltype = "spark-embedded" ]] ; then
        $_D java $EXTRA_JVM_CLI_ARGS \
            $(getConf $PRE.jvm)  \
            -cp $PIPELINES_JAR $CLASS \
            --datasetId=$dr \
            --config=$config
    fi

    PRE=index-sh-args.spark-cluster
    if [[ $ltype = "spark-cluster" ]] ; then
        $_D /data/spark/bin/spark-submit \
            --name "SOLR indexing for $dr" \
            $(toArg $PRE conf) \
            $(toArg $PRE num-executors) \
            $(toArg $PRE executor-cores) \
            $(toArg $PRE executor-memory) \
            $(toArg $PRE driver-memory) \
            --class $CLASS  \
            --master $SPARK_MASTER \
            --driver-java-options "-Dlog4j.configuration=file:/efs-mount-point/log4j.properties" \
            $PIPELINES_JAR \
            --datasetId=$dr \
            --config=$config
            # TODO set here an optional colorized log4j properties
    fi

    logStepEnd "Indexing" $dr $ltype $SECONDS
}

if [[ $dr = "all" ]] ; then
    log.info Dump dataset size for all
    java $EXTRA_JVM_CLI_ARGS -cp $PIPELINES_JAR au.org.ala.utils.DumpDatasetSize \
         --config=$config
fi # This should create /tmp/dataset-counts.csv

if ($do_all || $dwca_avro) ; then
    if [[ $dr != "all" ]] ; then
        for dr in "${dr[@]}"; do
            dwca-avro $dr
        done
    else
        rootDir=/data/biocache-load
        # !!!!!! need to run this command on one vm then run the script !!!!
        # rm -rf $rootDir/dr*.lockdir

        logStepStart "dwca-avro" all
        SECONDS=0
        tempDirs=()
        drDwcaList=($(ls -S $rootDir/dr*/dr*.zip))
        for drDwca in "${drDwcaList[@]}"; do
            filename=$(basename "$drDwca")
            datasetID="${filename%.*}"
            folder=$(dirname "$drDwca")
            if mkdir "$folder.lockdir"; then
                tempDirs+=("$folder.lockdir")
                # you now have the exclusive lock
                log.info "[DWCA-AVRO] Starting dwca avro conversion for $datasetID....."
                dwca-avro $datasetID
                log.info "[DWCA-AVRO] Finished dwca avro conversion for $datasetID."
            else
                log.info "[DWCA-AVRO] Skipping dwca avro conversion for $datasetID....."
            fi
        done

        logStepEnd "dwca-avro" all local $SECONDS
    fi
fi

if ($interpret || $do_all); then
    if [[ $dr != "all" ]] ; then
        for dr in "${dr[@]}"; do
            interpret $dr $TYPE
        done
    else
        while IFS=, read -r datasetID recordCount
        do
            log.info "Dataset = $datasetID and count = $recordCount"
            if [ "$recordCount" -gt "50000" ]; then
                if [ "$USE_CLUSTER" == "TRUE" ]; then
                    interpret $datasetID spark-cluster
                else
                    interpret $datasetID spark-embedded
                fi
            else
                interpret $datasetID local
            fi
        done < /tmp/dataset-counts.csv
    fi
fi

if ($uuid || $do_all); then
    if [[ $dr != "all" ]] ; then
        for dr in "${dr[@]}"; do
            uuid $dr $TYPE
        done
    else
        while IFS=, read -r datasetID recordCount
        do
            log.info "Dataset = $datasetID and count = $recordCount"
            if [ "$recordCount" -gt "50000" ]; then
                if [ "$USE_CLUSTER" == "TRUE" ]; then
                    uuid $datasetID spark-cluster
                else
                    uuid $datasetID spark-embedded
                fi
            else
                uuid $datasetID spark-embedded
            fi
        done < /tmp/dataset-counts.csv
    fi
fi

if ($export_latlng || $do_all); then
  if [[ $dr != "all" ]] ; then
      for dr in "${dr[@]}"; do
          export-latlng $dr $TYPE
      done
  else
      while IFS=, read -r datasetID recordCount
      do
          log.info "Dataset = $datasetID and count = $recordCount"
          if [ "$recordCount" -gt "50000" ]; then
              if [ "$USE_CLUSTER" == "TRUE" ]; then
                  export-latlng $datasetID spark-cluster
              else
                  export-latlng $datasetID spark-embedded
              fi
          else
              export-latlng $datasetID spark-embedded
          fi
      done < /tmp/dataset-counts.csv
  fi
fi

if ($sample || $do_all); then
    if [[ $dr != "all" ]] ; then
        sample $dr
    else
        while IFS=, read -r datasetID recordCount
        do
            sample $datasetID
        done < /tmp/dataset-counts.csv
    fi
fi

if ($sample_avro || $do_all); then
    if [[ $dr != "all" ]] ; then
        for dr in "${dr[@]}"; do
            sample-avro $dr $TYPE
        done
    else
        while IFS=, read -r datasetID recordCount
        do
            log.info "Dataset = $datasetID and count = $recordCount"
            if [ "$recordCount" -gt "50000" ]; then
                if [ "$USE_CLUSTER" == "TRUE" ]; then
                    sample-avro $datasetID spark-cluster
                else
                    sample-avro $datasetID spark-embedded
            fi
        else
            sample-avro $datasetID spark-embedded
        fi
        done < /tmp/dataset-counts.csv
    fi
fi

if ($index || $do_all); then
    if [[ $dr != "all" ]] ; then
        for dr in "${dr[@]}"; do
            index $dr $TYPE
        done
    else
        while IFS=, read -r datasetID recordCount
        do
            log.inf "Dataset = $datasetID and count = $recordCount"
            if [ "$recordCount" -gt "50000" ]; then
                if [ "$USE_CLUSTER" == "TRUE" ]; then
                    index $datasetID spark-cluster
                else
                    index $datasetID spark-embedded
                fi
            else
                index $datasetID java
            fi
        done < /tmp/dataset-counts.csv
    fi
fi
